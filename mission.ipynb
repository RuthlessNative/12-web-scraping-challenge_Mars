{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dependencies\n",
    "   import time\n",
    "   #import shutil\n",
    "   import requests\n",
    "   import pandas as pd\n",
    "   from splinter import Browser\n",
    "   #from IPython.display import Image\n",
    "   from bs4 import BeautifulSoup as bs\n",
    "   from webdriver_manager.chrome import ChromeDriverManager\n",
    "   ## Module used to connect Python with MongoDb\n",
    "   import pymongo"
   ]
  },
  {
   "source": [
    "Scrape the NASA Mars News Site and collect the latest News Title and Paragraph Text. Assign the text to variables that you can reference later."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup splinter to open test browser\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "## Give the browser time to open properly (\"Run all cells\" only)\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## URL of page to be scraped\n",
    "   url = 'https://mars.nasa.gov/news/'\n",
    "   \n",
    "   ## Opens test Chrome browser\n",
    "   browser.visit(url)\n",
    "   \n",
    "   ## Allow page to fully load (\"Run all cells\" only)\n",
    "   time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scrape page into Soup:\n",
    "\n",
    "## Create HTML object\n",
    "html = browser.html\n",
    "\n",
    "## Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Examine the results, then determine element that contains sought info. The original webpage may also be inspected on a browser\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \"content_title\" & \"article_teaser_body\" are both found under class \"list_text\"\n",
    "\n",
    "## soup.find_all results are returned as an iterable list\n",
    "ls_texts = soup.find_all(\"div\", class_=\"list_text\")\n",
    "ls_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the first item in ls_texts\n",
    "ls_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This For Loop returns title and paragraph text for each item in ls_texts (not needed to complete assignment)\n",
    "\n",
    "## Loop through returned results\n",
    "for text in ls_texts:\n",
    "     ## Error handling\n",
    "     try:\n",
    "         ## Identify and return title text\n",
    "         title_text = text.find('div', class_=\"content_title\").text\n",
    "         ## Identify and return paragraph text\n",
    "         p_text = text.find('div', class_='article_teaser_body').text\n",
    "\n",
    "         ## Print results only if title_text and p_text are available\n",
    "         if(title_text and p_text):\n",
    "             print('-------------')\n",
    "             print(f\"{title_text}: \")\n",
    "             print(p_text)\n",
    "             \n",
    "     except AttributeError as e:\n",
    "         print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### These lines grab the title and paragraph texts of the first headline per assignment instructions\n",
    "\n",
    "## Identify and return title text\n",
    "news_title = ls_texts[0].find('div', class_=\"content_title\").text\n",
    "## Identify and return paragraph text\n",
    "news_p = ls_texts[0].find('div', class_='article_teaser_body').text\n",
    "\n",
    "print(f\"{news_title}: {news_p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Close the test browser\n",
    "browser.quit()\n",
    "## Give the browser time to close properly (\"Run all cells\" only)\n",
    "time.sleep(5)"
   ]
  },
  {
   "source": [
    "JPL Mars Space Images - Featured Image\n",
    "\n",
    "\n",
    "Visit the url for JPL Featured Space Image here.\n",
    "\n",
    "\n",
    "Use splinter to navigate the site and find the image url for the current Featured Mars Image and assign the url string to a variable called featured_image_url.\n",
    "\n",
    "\n",
    "Make sure to find the image url to the full size .jpg image.\n",
    "\n",
    "\n",
    "Make sure to save a complete url string for this image."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup splinter to open test browser\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "## Give the browser time to open properly (\"Run all cells\" only)\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## URL of page to be scraped\n",
    "url = \"https://data-class-jpl-space.s3.amazonaws.com/JPL_Space/index.html\"\n",
    "\n",
    "## Opens test Chrome browser\n",
    "browser.visit(url)\n",
    "\n",
    "## Allow page to fully load (\"Run all cells\" only)\n",
    "time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Design an XPATH selector to grab the featured image\n",
    " \n",
    "## Right click HTML line holding button:\n",
    "## (<button class=\"btn btn-outline-light\"> FULL IMAGE</button>) -> 'Copy' -> 'Copy Full XPath'\n",
    "## then paste below\n",
    "xpath = \"/html/body/div[1]/div/a/button\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use splinter to Click the 'FULL IMAGE' button to bring up the full resolution image\n",
    "results = browser.find_by_xpath(xpath)\n",
    "img = results[0]\n",
    "img.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scrape the browser into soup and use soup to find the full resolution image of mars\n",
    "html = browser.html\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Examine the results, then determine element that contains sought info. The original webpage may also be inspected on a browser\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Save the image url to a variable called `img_url`\n",
    "## \"fancybox-image\" holds the Featured Image, \"src\" holds the url tail end\n",
    "img_url = soup.find(\"img\", class_=\"fancybox-image\")[\"src\"]\n",
    "\n",
    "featured_image_url = \"https://data-class-jpl-space.s3.amazonaws.com/JPL_Space/\" + img_url\n",
    "featured_image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Just for fun, not needed for assignment\n",
    " \n",
    " import shutil\n",
    " from IPython.display import Image\n",
    " \n",
    " # Use the requests library to download and save the image from the `img_url` above\n",
    "response = requests.get(featured_image_url, stream=True)\n",
    "with open('featured_img.png', 'wb') as out_file:\n",
    "    shutil.copyfileobj(response.raw, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Just for fun, not needed for assignment\n",
    "\n",
    "# Display the image with IPython.display\n",
    "from IPython.display import Image\n",
    "Image(url='featured_img.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Close the test browser\n",
    "browser.quit()\n",
    "## Give the browser time to close properly (\"Run all cells\" only)\n",
    "time.sleep(5)"
   ]
  },
  {
   "source": [
    "Mars Facts\n",
    "\n",
    "\n",
    "Visit the Mars Facts webpage here and use Pandas to scrape the table containing facts about the planet including Diameter, Mass, etc.\n",
    "\n",
    "\n",
    "Use Pandas to convert the data to a HTML table string."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the url\n",
    "url = \"https://space-facts.com/mars/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the `read_html` function in Pandas to automatically scrape any tabular data from a page\n",
    "tables = pd.read_html(url)\n",
    "## Show list of tables\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## The table in tables[0] holds Mars data\n",
    " ## Pandas also had a `to_html` method that can be used to generate HTML tables from DataFrames\n",
    "tables[0].to_html(\"Mars_Facts.html\") # this saves the filename in the parenthesis\n",
    "\n",
    "## or just use the code below to show the results:\n",
    "#tables[0].to_html()"
   ]
  },
  {
   "source": [
    "Mars Hemispheres\n",
    "\n",
    "\n",
    "Visit the USGS Astrogeology site here to obtain high resolution images for each of Mar's hemispheres.\n",
    "\n",
    "\n",
    "You will need to click each of the links to the hemispheres in order to find the image url to the full resolution image.\n",
    "\n",
    "\n",
    "Save both the image url string for the full resolution hemisphere image, and the Hemisphere title containing the hemisphere name. Use a Python dictionary to store the data using the keys img_url and title.\n",
    "\n",
    "\n",
    "Append the dictionary with the image url string and the hemisphere title to a list. This list will contain one dictionary for each hemisphere."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup splinter to open test browser\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "## Give the browser time to open properly (\"Run all cells\" only)\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## URL of page to be scraped\n",
    "url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "\n",
    "## Opens test Chrome browser\n",
    "browser.visit(url)\n",
    "\n",
    "## Allow page to fully load (\"Run all cells\" only)\n",
    "time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scrape page into Soup:\n",
    "\n",
    "## Create HTML object\n",
    "html = browser.html\n",
    "\n",
    "## Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Examine the results, then determine element that contains sought info. The original webpage may also be inspected on a browser\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \"content_title\" & \"article_teaser_body\" are both found under class \"list_text\"\n",
    "\n",
    "## soup.find_all results are returned as an iterable list\n",
    "ls_hemi_results = soup.find_all(\"div\", class_=\"item\")\n",
    "len(ls_hemi_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This For Loop finds the link of each Hemisphere in ls_hemi_results, goes to that link and parses it, then finds the \"Wide-Image\" link. Hemisphere title and Wide-Image links are stored in a dictionary.BufferError\n",
    "\n",
    "url = 'https://astrogeology.usgs.gov'\n",
    "\n",
    "## Create an empty list of dictionaries\n",
    "hemisphere_image_urls = []\n",
    "\n",
    "## Loop through returned results\n",
    "for item in ls_hemi_results:\n",
    "    ## Error handling\n",
    "    try:\n",
    "        ## Identify and return Hemisphere name text\n",
    "        hemi = item.find('div', class_=\"description\").h3.text\n",
    "\n",
    "        ## Identify and return Hemisphere link\n",
    "        link_1 = url + item.a[\"href\"]\n",
    "\n",
    "        ## Go to link_1 that contains the full image link\n",
    "        browser.visit(link_1)\n",
    "\n",
    "        ## Create HTML object of the Hemiphere page\n",
    "        link_2 = browser.html\n",
    "        ## Create BeautifulSoup object; parse with 'html.parser'\n",
    "        soup = bs(link_2, \"html.parser\")\n",
    "\n",
    "        ## Identify and return Hemisphere \"Wide-Image\" file link\n",
    "        link_3 = soup.find(\"img\", class_=\"wide-image\")[\"src\"]\n",
    "\n",
    "        ## Print results only if hemi, link_1 and link_2 are available\n",
    "        if(hemi and link_1 and link_3):\n",
    "            print('-------------')\n",
    "            print(hemi)\n",
    "            print(link_1)\n",
    "            #print(link_2)\n",
    "            print(link_3)\n",
    "\n",
    "        ## Append the Hemisphere name text and \"Wide-Image\" file link to the dictionary list\n",
    "        hemisphere_image_urls.append({\"title\" : hemi, \"img_url\" : url + link_3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemisphere_image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Close the test browser\n",
    "browser.quit()\n",
    "## Give the browser time to close properly (\"Run all cells\" only)\n",
    "time.sleep(5)"
   ]
  }
 ]
}